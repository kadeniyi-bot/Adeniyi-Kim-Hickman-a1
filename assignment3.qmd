---
format:
  pdf: default
  html: default
editor: visual
---

**ASSIGNMENT**

**WEB SCRAPPING**

**KEHINDE O. ADENIYI**

You may work in pairs or individually for this assignment. Make sure you join a group in Canvas if you are working in pairs. Turn in this assignment as an HTML or PDF file to ELMS. Make sure to include the R Markdown or Quarto file that was used to generate it. Include the GitHub link for the repository containing these files.

```{r}
library(xml2)
library(rvest)
library(purrr)
library(stringr)
library(tibble)
library(stopwords)
library(dplyr)
library(ggplot2)
```

## Web Scraping

In this assignment, your task is to scrape some information from Wikipedia. We start with the following page about Grand Boulevard, a Chicago Community Area.

<https://en.wikipedia.org/wiki/Grand_Boulevard,_Chicago>

The ultimate goal is to gather the table "Historical population" and convert it to a `data.frame`.

**Question**

As a first step, read in the html page as an R object. Extract the tables from this object (using the `rvest` package) and save the result as a new object. Follow the instructions if there is an error. Use `str()` on this new object -- it should be a list. Try to find the position of the "Historical population" in this list since we need it in the next step.

```{r}
## read the html page as an R object.
boul_url <-'https://en.wikipedia.org/wiki/Grand_Boulevard,_Chicago'
boul_url

boul_page <- read_html(boul_url)
boul_page
```

```{r}
##Extracting the tables from this object and save the result as a new object.

boul_elem <- html_elements(boul_page, "table")

boul_tables <- map(boul_elem, html_table, fill = TRUE)
boul_tables

str(boul_tables, max.level = 2)
```

**Answer**

The Historical population table is at position 2 in the list of tables that were extracted.

**Question**

Extract the "Historical population" table from the list and save it as another object. You can use subsetting via `[[â€¦]]` to extract pieces from a list. Print the result.

```{r}
## Extracting the "Historical population" table from the list
historical_pop <- boul_tables[[2]]
historical_pop
```

You will see that the table needs some additional formatting. Keep only want rows and columns with actual values.

```{r}
## formatting to remove extra rows
historical_pop <- historical_pop[2:10, -3]
historical_pop
```

## Expanding to More Pages

**Question**

That's it for this page. However, we may want to repeat this process for other community areas. The Wikipedia page <https://en.wikipedia.org/wiki/Grand_Boulevard,_Chicago> has a section on "Places adjacent to Grand Boulevard, Chicago" at the bottom. Can you find the corresponding table in the list of tables that you created earlier? Extract this table as a new object.

```{r}
##Extracting the "Places adjacent to Grand Boulevard, Chicago" section
places_adj <- boul_tables[[4]]
places_adj
```

Then, grab the community areas east of Grand Boulevard and save them as a character vector. Print the result.

```{r}
east_grand_boul <- c("Oakland, Chicago", "Kenwood, Chicago", "Hyde Park, Chicago")
east_grand_boul
```

**Question**

We want to use this list to create a loop that extracts the population tables from the Wikipedia pages of these places. To make this work and build valid urls, we need to replace empty spaces in the character vector with underscores. The resulting vector should look like this: "Oakland,\_Chicago" "Kenwood,\_Chicago" "Hyde_Park,\_Chicago"

```{r}
# Replacing the empty spaces with underscores for east Chicago
east_grand_boul_change <- gsub(" ", "_", east_grand_boul)
east_grand_boul_change
```

Build a loop to grab the population tables from each page. Add columns to the original table using `cbind()`.

```{r}

historical_population <- historical_pop


for (i in east_grand_boul_change) {
  chicago_url <- paste0("https://en.wikipedia.org/wiki/", i)
  print(chicago_url)

  ## read the Chicago URL
  chicago_base <- read_html(chicago_url)
  
  ## get the tables and check the number of variables and observations
  chicago_tables <- html_elements(chicago_base, "table")
  chiboul_tables <- map(chicago_tables, html_table, fill = TRUE)
  
  
  if (length(chiboul_tables) >= 4) {
    hist_population <- chiboul_tables[[4]]
    
    
    if (ncol(hist_population) >= 2 && nrow(hist_population) >= 11) {
      hist_population <- hist_population[2:10, -3] 
      
      # removing the underscores
      colnames(hist_population)[4] <- gsub("_", " ", i)
      
      historical_population <- cbind(historical_population, hist_population[, 2])
    } else {
      warning(paste("Tables not needed:", i))
    }
  } else {
    warning(paste("The number tables are not identical:", i))
  }
}

historical_population
```

## Scraping and Analyzing Text Data

**Question**

Suppose we wanted to take the actual text from the Wikipedia pages instead of just the information in the table. Our goal in this section is to extract the text from the body of the pages, then do some basic text cleaning and analysis.

First, scrape just the text without any of the information in the margins or headers. For example, for "Grand Boulevard", the text should start with, "**Grand Boulevard** on the [South Side](https://en.wikipedia.org/wiki/South_Side,_Chicago "South Side, Chicago") of [Chicago](https://en.wikipedia.org/wiki/Chicago "Chicago"), [Illinois](https://en.wikipedia.org/wiki/Illinois "Illinois"), is one of the ...". Make sure all of the text is in one block by using something like the code below (I called my object `description`).

```{r}
# description <- description %>% paste(collapse = ' ')

##scraping just the text without any of the information in the margins or headers

boul_url <- "https://en.wikipedia.org/wiki/Grand_Boulevard,_Chicago"
boul_page <- read_html(boul_url)

description <- boul_page %>%
  html_elements("p") %>%
  html_text() %>%
  paste(collapse = " ") %>%
  str_replace_all("\\[[^\\]]+\\]", "") %>%
  str_squish()


cat(substr(description, 1, 400))

```

**Question**

Using a similar loop as in the last section, grab the descriptions of the various communities areas. Make a tibble with two columns: the name of the location and the text describing the location.

```{r}
east_grand_boul_change <- c("Oakland,_Chicago", "Kenwood,_Chicago", "Hyde_Park,_Chicago")

communities_des <- map_df(east_grand_boul_change, function(loc) {
  url <- paste0("https://en.wikipedia.org/wiki/", loc)
  
  communities_areas <- read_html(url) %>%
    html_elements("p") %>%
    html_text() %>%
    paste(collapse = " ") %>%
    str_replace_all("\\[[^\\]]+\\]", "") %>%
    str_squish()
  
  tibble(location = gsub("_", " ", loc), description = communities_areas)
})
cat(substr(communities_des$description[1], 1, 450))
```

Let's clean the data using `tidytext`. If you have trouble with this section, see the example shown in <https://www.tidytextmining.com/tidytext.html>

```{r}
library(tidytext)
```

**Question**

Create tokens using `unnest_tokens`. Make sure the data is in one-token-per-row format. Remove any stop words within the data. What are the most common words used overall?

```{r}
tokens_unrest <- communities_des %>%
  unnest_tokens(common_words, description)

common_words <- tokens_unrest %>%
  anti_join(stop_words, by = c("common_words" = "word"))
common_words 
```

**Answer**

Some of the most common words used are community, Oakland, located, USA, South Chicago, Illinois, among others.

**Question**

Plot the most common words within each location. What are some of the similarities between the locations? What are some of the differences?

```{r}

## the most common words used
word_freq <- common_words %>%
  count(location, common_words, sort = TRUE)

word_freq

most_used_words <- word_freq  %>%
  group_by(location) %>%
  slice_max(n, n = 15) %>%
  ungroup()
most_used_words

## plotting the graph

ggplot(most_used_words, aes(x = common_words, y = n)) +
  geom_col() +
  facet_wrap(~ location, scales = "free") +
  theme(axis.text.x = element_text(angle = 30, hjust = 2)) +
  labs(title = "Top 15 Words by Location", x = "Common_Words", y = "Word_Counts")

```

**Answer**

For Hyde Park, Chicago and Kenwood, the words Chicago and Hyde came up a lot. For the differences, words like home and city were not that frequent among them.
